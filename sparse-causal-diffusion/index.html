<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Causality in Video Diffusers is Separable from Denoising</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <div class="container">
        <!-- Header -->
        <header>
            <h1 class="title">Causality in Video Diffusers is Separable from Denoising</h1>
            <div class="authors">
                <span class="author">Xingjian Bai<sup>1,2</sup></span>
                <span class="author">Guande He<sup>3</sup></span>
                <span class="author">Zhengqi Li<sup>2</sup></span>
                <span class="author">Eli Shechtman<sup>2</sup></span>
                <span class="author">Xun Huang<sup>3</sup></span>
                <span class="author">Zongze Wu<sup>2</sup></span>
            </div>
            <div class="affiliations">
                <span class="affiliation"><sup>1</sup>Massachusetts Institute of Technology</span>
                <span class="affiliation"><sup>2</sup>Adobe Research</span>
                <span class="affiliation"><sup>3</sup>Morpheus AI</span>
            </div>

            <div class="links">
                <a href="https://arxiv.org/abs/2602.10095" class="link-button">
                    <i class="fas fa-file-pdf"></i> Paper
                </a>
                <a href="https://github.com/xingjian-bai/sparse-causal-diffusion" class="link-button">
                    <i class="fab fa-github"></i> Code
                </a>
            </div>
        </header>

        <!-- Abstract -->
        <section class="abstract">
            <h2>Abstract</h2>
            <p>
                Causality &mdash; referring to temporal, uni-directional cause-effect relationships between components &mdash; underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.
            </p>
        </section>

        <!-- Teaser Figure -->
        <section class="figure">
            <img src="teaser.png" alt="Teaser figure illustrating the separability of causality from denoising in video diffusion models.">
            <figcaption>
                <strong>Causality in autoregressive video diffusion models is separable from the denoising process.</strong>
                The prevailing design of causal diffusion models for visual generation performs causal attention densely across all layers and all denoising steps (left). However, we uncover two important observations (right): (1) early denoiser layers share highly repetitive computation across denoising steps (<span class="fig-blue">blue</span>); (2) deep layers primarily attend to intra-frame tokens, with sparse cross-frame connections (<span class="fig-red">red</span>).
            </figcaption>
        </section>

        <!-- Observation 1 -->
        <section class="observation">
            <h2>Observation 1: Redundant Computation Across Denoising Steps</h2>
            <p class="observation-text">
                When generating a frame, early and middle layers of video diffusion models produce nearly identical features across the entire diffusion trajectory. As shown in the cosine similarity matrices below, the 15th-layer features remain highly correlated across all 50 denoising steps. PCA visualizations further confirm that these layers capture global layout and motion from context frames, yet redundantly recompute them at every denoising step. This suggests that early-layer computation can be amortized and shared across steps without loss of information.
            </p>
            <div class="observation-figure">
                <img src="obs1_redundancy.jpg" alt="Cosine similarity matrices and PCA patterns showing redundant computation across denoising steps in early layers.">
                <figcaption><strong>(a)</strong> Cosine similarity of 15th-layer features across 50 denoising steps. <strong>(b)</strong> PCA patterns of 15th-layer features remain stable across the diffusion process. Visualizations obtained from probing autoregressive-finetuned WAN 2.1 (1.4B).</figcaption>
            </div>
        </section>

        <hr class="section-divider">

        <!-- Observation 2 -->
        <section class="observation">
            <h2>Observation 2: Sparse Cross-Frame Attention in Deep Layers</h2>
            <p class="observation-text">
                Deeper layers exhibit a markedly different behavior: they barely attend to past frames. Despite being trained with dense causal attention masks, the cross-frame attention weights in deeper layers become increasingly sparse with depth. Layers 28 and 29 focus almost exclusively on intra-frame tokens, performing per-frame rendering rather than temporal reasoning. This natural emergence of temporal sparsity motivates a clean architectural separation between cross-frame reasoning and per-frame denoising.
            </p>
            <div class="observation-figure">
                <img src="obs2_attention.jpg" alt="Attention weight visualizations showing sparse cross-frame attention in deep layers.">
                <figcaption>Attention weight maps across layers. Early layers (0&ndash;10) attend broadly to context frames, while deep layers (20&ndash;29) exhibit sparse cross-frame connections and focus on intra-frame rendering. Visualizations obtained from probing autoregressive-finetuned WAN 2.1 (1.4B).</figcaption>
            </div>
        </section>

        <hr class="section-divider">

        <!-- Separable Causal Diffusion -->
        <section class="observation">
            <h2>Separable Causal Diffusion</h2>
            <p class="observation-text">
                Motivated by these observations, we propose Separable Causal Diffusion (SCD), which explicitly decouples temporal reasoning from iterative denoising. An autoregressive causal encoder operates <em>once per frame</em>, replacing the redundant early layers and producing a compact temporal context&mdash;analogous to the backbone of autoregressive language models. A lightweight diffusion decoder then renders each frame independently, conditioned on this context, with <em>no access</em> to past frames. This separation allows the encoder to be made larger without increasing per-step cost, while the decoder remains efficient across multiple denoising steps.
            </p>
            <div class="observation-figure">
                <img src="scd_architecture.jpg" alt="Architecture diagram of Separable Causal Diffusion showing the causal encoder and frame-wise diffusion decoder." class="architecture-img">
                <figcaption>The SCD architecture. A causal encoder (<span class="fig-blue">blue</span>, &times;1) summarizes temporal context once per frame. A frame-wise diffusion decoder (<span class="fig-red">red</span>, &times;T) renders each frame through iterative denoising, conditioned on the causal context.</figcaption>
            </div>
        </section>

        <hr class="section-divider">

        <!-- Video Gallery -->
        <section class="video-gallery">
            <h2>Text-to-Video Generated Samples</h2>
            <p class="gallery-description">
                Our model generates high-quality 480P videos with an initial latency of ~0.29 seconds, after which frames are generated in a streaming fashion at ~11.1 FPS on a single H100 GPU. Below, we show text-to-video samples generated by our fine-tuned 1.3B Separable Causal Diffusion model with a 25-layer causal encoder and 10-layer diffusion decoder. Click on any video to view in full size with the text prompt.
            </p>
            <div id="videoGrid" class="video-grid">
                <!-- Videos will be inserted here by JavaScript -->
            </div>
        </section>

        <!-- BibTeX -->
        <section class="bibtex">
            <h2>BibTeX</h2>
            <pre><code>@article{bai2026causality,
  title={Causality in Video Diffusers is Separable from Denoising},
  author={Bai, Xingjian and He, Guande and Li, Zhengqi and Shechtman, Eli and Huang, Xun and Wu, Zongze},
  journal={arXiv preprint},
  year={2026}
}</code></pre>
        </section>

        <!-- Footer -->
        <footer>
            <p>Â© 2026 Xingjian Bai et al. All rights reserved.</p>
        </footer>
    </div>

    <!-- Modal for video viewer -->
    <div id="videoModal" class="modal">
        <div class="modal-content">
            <span class="close">&times;</span>
            <video id="modalVideo" controls></video>
            <div class="modal-caption">
                <h3>Prompt</h3>
                <p id="modalPrompt"></p>
            </div>
        </div>
    </div>

    <script src="script.js"></script>
</body>
</html>